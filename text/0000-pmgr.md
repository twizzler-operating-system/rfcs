- Feature Name: paging_and_persistence
- Start Date: 2022-06-03
- RFC PR: [twizzler-rfcs/rfcs#0000](https://github.com/twizzler-operating-system/rfcs/pull/0000)
- Twizzler Issue: [twizzler-operating-system/twizzler#0000](https://github.com/twizzler-operating-system/twizzler/issues/0000)

# Summary
[summary]: #summary

This RFC introduces the pager system, and specifies the core interaction between the pager system and the kernel. The
purpose of the pager is to have a mechanism to allow for an object to exist outside the kernel's knowledge (e.g. on an
SSD or out on the network) and for that object to be brought into memory when needed. The kernel interacts with the
pager by submitting page and object info requests on a shared queue.

In addition to specifying core pager behavior, this RFC outlines a set of phases in which the pager's functionality will
be expanded. In particular, we'll expand in the future on 1) pager-userspace interaction, 2) pager's interaction with
copy-from instructions and copy-on-write, 3) pager's interaction with a larger, network-enabled runtime, and 4) pager's
support and interaction for transactional operations on objects. We choose to expand on those in the future and not now
to keep this RFC smaller as it explores the core pager functionality.

# Motivation
[motivation]: #motivation

This mechanism is vital to support any amount of persistence or sharing, as we need some established
mechanism for the kernel to defer to userspace in the case that an object doesn't exist in memory
but does elsewhere. This also directly supports persistent objects, and combines directly with the
twizzler-nando crate. We expect the result to be, 1) a stable API for the kernel to defer to
userspace, 2) a system-level userspace service that handles those requests and can interact with
storage devices to load objects into memory (and sync them back to storage), and 3) a stable API for
other applications to submit requests to the pager (not included in this RFC).

# Guide-level explanation
[guide-level-explanation]: #guide-level-explanation

The *pager* is a userspace service that is tightly coupled with the kernel, and acts as a way for
the kernel to discover objects that it doesn't know about and to gain access to pages holding data
for those objects. One primary aspect of this is providing a way to use *stable storage* (any device
whose data persists) in Twizzler, with the pager moving pages back and forth between (e.g.) an SSD
and memory.

# Reference-level explanation
[reference-level-explanation]: #reference-level-explanation

We'll break the discussion of the core pager functionality into 4 parts: negotiating physical memory, object
information, object page data, and persistence. We will also discuss setup and teardown.

## Setup

The kernel expects the init program to communicate to it a pair of object IDs that are each Queue
objects---the *kernel request queue*, and the *pager request queue*. These objects allow for
communication between the kernel and userspace, where the userspace portion is expected to be a
binary program started by init that handles kernel paging requests.

This coordination on which objects to use for queues is done via the kernel using the `sys_new_handle` system call. The
init program calls this function twice to record first the kernel request queue, and second the pager request queue.
Once these queues' IDs are both told to the kernel, the kernel starts its internal paging mechanisms (implementation details not
outlined in this RFC).

The kernel pager request queue's submission and completion types are:

```{rust}
enum KernelRequest {
    ObjectInfoReq(ObjID),
    PageDataReq(ObjID, [ObjectRange; A]),
    DramRel(usize),
    DramPages([PhysRange; B]),
    Evict(ObjID, [ObjectRange; C]),
    Sync(ObjID, [ObjectRange; C]),
}

enum KernelCompletion {
    Success,
    ObjectInfo(ObjectInfo),
    PageInfo([PhysRange; D]),
    DramPages([PhysRange; E]),
    Err(PagerReqError),
}
```

The pager's request queue has the following types:

```
enum PagerRequest {
    PageInfo([(PhysRange, ObjectRange); F]),
    ObjectInfo(ObjectInfo),
    DramReq(usize),
    DramPages([PhysRange; G]),
    Evict(ObjID, [ObjectRange; H])
}

enum PagerCompletion {
    Success,
    DramPages([PhysRange, I]),
    Err(KernelReqError),
}
```

The pager can directly submit information about pages and objects to the kernel before the kernel
even asks for it, allowing for prefetching (possibly in the background, or application-driven). The
kernel can choose to ignore these prefetching requests.

## Memory Management

To allow for the pager to implement some storage mechanism that does DMA transfer, the kernel allows
for DRAM-related requests over these queues. At any time the kernel can offer up unused pages of
DRAM for the pager to gain ownership over, and the pager can also issue a request for additional
pages. When requesting additional pages, the kernel is allowed to refuse if low on memory. The
kernel can also issue a request to the pager to release DRAM pages. The pager is allowed to refuse.
For this reason, the kernel should not pass ownership of all DRAM to the pager.

### Access to Physical Memory

Since the pager exists in userspace, it does not have access to the DRAM pages that are given to it
by the kernel. While in most operation, it does not need to read DRAM pages (the pager can submit
DMA commands without access to the memory), occasionally it might want to read page contents. To do
this, the pager can simply invent an object ID and submit PageInfo requests to the kernel to
temporarily map pages into accessable memory.

### Request Types

When communicating lists of pages back and forth, both sides use an array of `PhysRange`.
The `PhysRange` struct looks like: 

```{rust}
struct PhysRange {
    start: u64,
    len: u32,
    _flags: u32,
}
```

Thus each side can communicate sets of contiguous pages with each other. The flags field is currently unused, but when
used should be replaced with a newtype that has the same representation as a u32.

## Object Info Requests

The object info struct looks like:

```{rust}
struct ObjectInfo {
    lifetime: LifetimeType,
    backing: BackingType,
}
```

This struct will be expanded over-time.

## Eviction

To support eviction, the kernel can inform the pager that a page must by written back to stable
storage. The pager must honor the kernel's command and must assume the page is dirty. However, this
is meant to be used as a last resort. The kernel can also submit a *request* that the pager free up
some number of pages. If a page returned to the kernel is dirty, the kernel will submit an
eviction notice for that page to inform the pager that it needs to write-back that page.

Note that there is a distinction between "Evict" and "Sync". The former alone requires that data be stored but not
overwrite object data that may have been modified in-memory. This is required to support transactional semantics well.
We can combine Sync and Evict to recover standard OS eviction semantics. Sync by itself just syncs and doesn't
evict. Finally, the pager can request the kernel evict object memory.

## Object Page Requests

The kernel can request the pager to fill a DRAM page with data from stable storage and return it to the kernel. The 
`ObjectRange` struct looks like: 

```{rust}
struct ObjectRange {
    start: u64,
    len: u32,
    _flags: u32,
}
```

Thus each side can communicate sets of contiguous byte-granularity regions of an object with each other. The flags field is currently unused, but when
used should be replaced with a newtype that has the same representation as a u32. Note that this means syncing and eviction can
happen at byte-level granularity, however it's an error to submit a page request that is non-page-aligned. What does
byte-level granularity eviction mean? It means that region needs to reset to stable-storage, including bytes that may
not cover a full page.

When the kernel requests information about a page, it sends a set of `ObjectRange` structs for a given object ID. The
pager responds with a set of `PhysRange` structs. The pager's response does not need to fulfill the entire request, as
long as at least one page is filled (probably the first one). The kernel reads out physical pages, in order as specified
by the completion, and maps them in sequence into the object's page tree.

The pager, at any time, can prefetch (if it has memory) and inform the kernel of its prefetching work. It does this via
the tuple `(PhysRange, ObjectRange)` in the `PageInfo` variant of the `PagerRequest` enum.

# Drawbacks
[drawbacks]: #drawbacks

Ultimately, the system needs a manager of stable storage and a mechanism to move object memory
safely to and from that storage.

# Rationale and alternatives
[rationale-and-alternatives]: #rationale-and-alternatives

The pager is designed to fit into userspace, because it cannot be easily implemented in-kernel. This
is because the drivers for devices that contain stable storage will be implemented in userspace, so
the pager (which sits above them) should be as well. As a result, most of the design decisions are
in-place to avoid the overheads of the kernel boundary crossings (escrow for DRAM pages, hinting,
etc.). The choice to bypass the object system for NVM is done for this same reason.

Another major component of this is the idea that a user application could submit syncing requests to
the pager in a more fine-grained way than "sync(2)" or "sync this object". The hope is to give
enough control to a transaction manager that could then efficiently sync only the pages that it
needs to to ensure failure-atomicity.

However, syncing via the kernel is slow. Ideally, userspace would be able to request the pager sync without involving
the kernel at all. We plan to do this, see Future Possibilities. However, even with that system in-place, we want the
kernel to be able to submit sync requests. This is because it probably needs this support anyway, but we also need to
allow the kernel to send Sync requests to the pager on behalf of applications that may not be able to communicate with
the pager directly for some reason.

# Prior art
[prior-art]: #prior-art

This approach is similar to standard microkernel designs where the functionality of paging and
drivers for stable storage cannot be added to the kernel.

# Unresolved questions
[unresolved-questions]: #unresolved-questions

- What should the values of the constants A-I, above, be?
- What should the format of the on-stable-storage data be?
- Connection to larger runtime: we need to write a whole additional RFC about this.
- Currently we don't support good semantics for interacting with the copy-from API that the Twizzler kernel provides. A future RFC will expand on this.

# Future possibilities
[future-possibilities]: #future-possibilities

## Transactional support

We will need to expand this interface in several ways:

 1. Support ordering restrictions between kernel sync requests. This may be in the form of a barrier queue entry, or
    more complex semantics.
 2. Transactional support for object creation requires the pager be involved at object creation. This will require an
    expansion of the communication protocol.

## Connection to Applications

We plan to allow applications to connect to the pager and submit requests for synchronizing pages
back to stable storage. This involves applications connecting to the pager via a queue, allowing the
application to submit object IDs and page ranges for syncing. This will be supported via the
SyncRequest type, which can optionally act as a barrier or a flush to allow the pager to move requests around
and batch them for performance and still allow the application some control on ordering and forcing.

## Non-volatile Memory (NVM)

While NVM[^1] is not required for this system, it can be used as an accelerator for persistent
operations. To support this, the kernel can map NVM directly into the address space of the pager.
Once the queue are connected, the kernel will submit a sequence of MemoryMap commands to the pager
to announce to it the topology of the NVM in the system. After this, the pager may use the kaction
system call on it's VM handle to map the NVM into its address space whereever it likes. Device-level
access to the NVM can be handled via the device manager like any other device.

[^1]: Here, we're specifically talking about NVM that sits on the memory bus. Other forms that can
    be accessed (e.g.) via NVMe are treated more like an SSD.